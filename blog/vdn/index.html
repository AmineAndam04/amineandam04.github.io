<!DOCTYPE html>
<html lang="en-us"><head><meta charset="utf-8">
<meta http-equiv="content-type" content="text/html">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title itemprop="name">VDN:  Value Decomposition Networks for cooperative multi-agent learning | RL blog</title>
<meta property="og:title" content="VDN:  Value Decomposition Networks for cooperative multi-agent learning | RL blog" />
<meta name="twitter:title" content="VDN:  Value Decomposition Networks for cooperative multi-agent learning | RL blog" />
<meta itemprop="name" content="VDN:  Value Decomposition Networks for cooperative multi-agent learning | RL blog" />
<meta name="application-name" content="VDN:  Value Decomposition Networks for cooperative multi-agent learning | RL blog" />
<meta property="og:site_name" content="" />

<meta name="description" content="">
<meta itemprop="description" content="" />
<meta property="og:description" content="" />
<meta name="twitter:description" content="" />

<meta property="og:locale" content="en-us" />
<meta name="language" content="en-us" />

  <link rel="alternate" hreflang="en" href="https://amineandam04.github.io/blog/vdn/" title="" />



  <meta itemprop="image" content="https://amineandam04.github.io/" />
  <meta property="og:image" content="https://amineandam04.github.io/" />
  <meta name="twitter:image" content="https://amineandam04.github.io/" />
  <meta name="twitter:image:src" content="https://amineandam04.github.io/" />





<meta name="generator" content="Hugo 0.123.3">

    

    <link rel="canonical" href="https://amineandam04.github.io/blog/vdn/">
    <link href="/style.min.d43bc6c79baa87f006efb2b92be952faeedeb1a3ab626c1d6abda52eae049355.css" rel="stylesheet">
    <link href="/code-highlight.min.706d31975fec544a864cb7f0d847a73ea55ca1df91bf495fd12a177138d807cf.css" rel="stylesheet">

    
    <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/icons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/icons/favicon-16x16.png">
    <link rel="mask-icon" href="/icons/safari-pinned-tab.svg">
    <link rel="shortcut icon" href="/favicon.ico">




<link rel="manifest" href="https://amineandam04.github.io/site.webmanifest">

<meta name="msapplication-config" content="/browserconfig.xml">
<meta name="msapplication-TileColor" content="#2d89ef">
<meta name="theme-color" content="#434648">

    
    <link rel="icon" type="image/svg+xml" href="/icons/favicon.svg">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"
    integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous" />


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"
    integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4"
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous" onload="renderMathInElement(document.body, {
      delimiters: [
        {left: '$$', right: '$$', display: true},
        {left: '$', right: '$', display: false}
      ]
    });"></script>

    </head>
<body data-theme = "" class="notransition">

<script src="/js/theme.min.8961c317c5b88b953fe27525839672c9343f1058ab044696ca225656c8ba2ab0.js" integrity="sha256-iWHDF8W4i5U/4nUlg5ZyyTQ/EFirBEaWyiJWVsi6KrA="></script>

<div class="navbar" role="navigation">
    <nav class="menu" aria-label="Main Navigation">
        <a href="https://amineandam04.github.io/" class="logo">
            <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" 
viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" 
stroke-linejoin="round" class="feather feather-home">
<title></title>
<path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path>
<polyline points="9 22 9 12 15 12 15 22"></polyline>
</svg>
        </a>
        <input type="checkbox" id="menu-trigger" class="menu-trigger" />
        <label for="menu-trigger">
            <span class="menu-icon">
                <svg xmlns="http://www.w3.org/2000/svg" width="25" height="25" stroke="currentColor" fill="none" viewBox="0 0 14 14"><title>Menu</title><path stroke-linecap="round" stroke-linejoin="round" d="M10.595 7L3.40726 7"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 3.51488L3.49301 3.51488"></path><path stroke-linecap="round" stroke-linejoin="round" d="M10.5096 10.4851H3.49301"></path><path stroke-linecap="round" stroke-linejoin="round" d="M0.5 12.5V1.5C0.5 0.947715 0.947715 0.5 1.5 0.5H12.5C13.0523 0.5 13.5 0.947715 13.5 1.5V12.5C13.5 13.0523 13.0523 13.5 12.5 13.5H1.5C0.947715 13.5 0.5 13.0523 0.5 12.5Z"></path></svg>
            </span>
        </label>

        <div class="trigger">
            <ul class="trigger-container">
                
                
                <li>
                    <a class="menu-link " href="/">
                        Home
                    </a>
                    
                </li>
                
                <li>
                    <a class="menu-link " href="/blog/">
                        Post
                    </a>
                    
                </li>
                
                <li class="menu-separator">
                    <span>|</span>
                </li>
                
                
            </ul>
            <a id="mode" href="#">
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-sunny" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>LIGHT</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
                <svg xmlns="http://www.w3.org/2000/svg" class="mode-moon" width="21" height="21" viewBox="0 0 14 14" stroke-width="1">
<title>DARK</title><g><circle cx="7" cy="7" r="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></circle><line x1="7" y1="0.5" x2="7" y2="2.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="2.4" x2="3.82" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="0.5" y1="7" x2="2.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="2.4" y1="11.6" x2="3.82" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="7" y1="13.5" x2="7" y2="11.5" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="11.6" x2="10.18" y2="10.18" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="13.5" y1="7" x2="11.5" y2="7" fill="none" stroke-linecap="round" stroke-linejoin="round"></line><line x1="11.6" y1="2.4" x2="10.18" y2="3.82" fill="none" stroke-linecap="round" stroke-linejoin="round"></line></g></svg>
            </a>
        </div>
    </nav>
</div>

<div class="wrapper post">
    <main class="page-content" aria-label="Content">
        <article>
            <header class="header">
                <h1 class="header-title">VDN:  Value Decomposition Networks for cooperative multi-agent learning</h1>
                
                
                <div class="post-meta">
                    <time datetime="2024-07-03T19:52:42&#43;01:00" itemprop="datePublished"> Jul 3, 2024 </time>
                </div>
                
            </header>
            
            <div class="page-content">
                <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<p>The VDN paper presents an algorithm to solve cooperative multi-agent reinforcement learning  with common rewards (team reward). In this settings, several agents are working together to maximise the (discounted) future rewards. Each agent receive a local obervation ( partial-observabiliy) that helps him take apropriate actions. It&rsquo;s worth mentioning that the VDN algorithm considers only discrete actions.</p>
<p>The cooperative MARL problem can be solved using single-agent RL algorithms such as DQN or PPO by considering a single-agent whoses state space is the contatenated state of the agents and an action space that is the combination of the action spaces of the agents. Alternatively, each agent can be seen as an idependant agent runing his own PPO algorithm for example.</p>
<p>The two aformentioned algorithms can be problematic. The first one can suffer from state space and action space explosion, as there are exponential with increasing number of agents. The second approach suffers from non-stationarity and credit assignement.</p>
<p>We consider that we have a finate set of agents </p>
$$ I = {1, ...,n} $$
<p> each agent receives an obsevation $O_i$ and a reward $R_i$. In our case, all agents recieve the same reward $ R_i = R $. Our goal is to maximize the expected cumulative discounted reward $\sum_{\tau=t} \gamma^{\tau - t} r_{\tau}$</p>
<p>We formalize the Dec-POMDP problem as
A Dec-POMDP is formally defined by the tuple $(N,{A_i}<em>{i=1, \ldots,  N}, {O_i}</em>{i=1, \ldots,  N}, R)$, where:</p>
<ul>
<li>$N$: The number of agents in the environment.</li>
<li>${A_i}$: The set of actions available to agent $i$. $a = {a_i}_{i=1, \ldots,  N}$ is the joint action.</li>
<li>${O_i}$: The set of observations available to agent $i$. Each agent receives observations that provide partial information about the state. $o = {o_i}_{i=1, \ldots,  N}$ is the joint observation.</li>
<li>$R$: The reward function $R(o,a)$, which provides a scalar reward based on the current observation and the actions of all agents. This reward is  shared among all agents.</li>
</ul>
<p>To solve this problem we use the Q-Learning paradigm. There is two approach in solving this problem:</p>
<p>The first approach is to use single-agent reinforcement learning algorithm. This consist in considering that there is an agent who receives the joint observation (e.g. the concatenation of individual observation of each agent) and its action is the joint actions of all the agents. This can be done using Q-learnig methods where the loss function is the following:</p>
$$
L(\theta) = \frac{1}{|B|} \sum_{(o_t, a_t, r_t, o'_t) \in B} (y_t - Q(o_t, a_t; \theta))
$$
$$
y_t = \begin{cases}
    r_t & \text{if } o_{t+1} \text{ is terminal} \\
    r_t + \gamma \max_{a'} Q(o_{t+1}, a'; \theta) & \text{otherwise}
\end{cases}
$$
<p>The second approach is to use independent Q-learning. This means that each agent will instantiate his own Q-learning algorithm relying on his local observation $o_i$ and the joint reward. This means that we have $N$ loss function. For each agent we have the following loss function:</p>
$$
L_i(\theta_i) = \frac{1}{|B|} \sum_{(o_i^t, a_i^t, r_i^t, o_{i}^{'t}) \in B} (y_i^t - Q_i(o_i^t, a_i^t; \theta))
$$
<p>where:
</p>
$$
y_i^t = \begin{cases}
    r_t & \text{if } o_i^{t+1} \text{ is terminal} \\
    r_t + \gamma \max_{a'_i} Q(o_i^{t+1}, a'_i; \theta_i) & \text{otherwise}
\end{cases}
$$
<p>The Q-network in the single agent case takes as an input the concatenated observations and actions of each agent. This can be very problematic as we have to deal with very large inputs. Moreover, the input will grow exponentially with the numbe of agents. However, when back propagating we use the team reward $R$ which reflects and strongly correlate with the joint action $a$. This is not the case with the independant Q-learning losses as each agent backpropagate $R$, but we don&rsquo;t have any guarantee that $R$ reflects the quality of the individual action $a_i$ as all agent contributed to have $R$. But the independent Q-learning enables us to use just the local observation $o_i$, thus we can train those networks more effeciently avoiding state explosion problems. it also helps when we want to deploy the networks.</p>
<p>The idea of VDN paper is two combine the two approaches in order to take advantage of both approaches. First, we want to train our networks using only local observations $o_i$ avoiding large inputs, thus we train these networks $Q(o_i^{t+1}, a&rsquo;_i; \theta_i)$. Second we want to backpropagate using the right reward signal. This can be done only when working with $Q(o_t, a_t; \theta)$. So we make the following assumption :</p>
$$
Q(o_t, a_t; \theta) = \sum_{i \in I} Q(o_t^i, a_t^i; \theta_i)
$$
<p>This means that we don&rsquo;t propagate each individual Q-network, but instead we backpropagate through the sum the individual Q-networks.</p>
<p>The loss function of the VDN algorithm is:
</p>
$$
L(\theta) = \frac{1}{|B|} \sum_{(h_t, a_t, r_t, h_{t+1}) \in B} \left( r_t + \gamma \max_{a \in A} Q(h_{t+1}, a; \bar{\theta}) - Q(h_t, a_t; \theta) \right)^2 \tag{9.43}
$$
<p>with</p>
$$
Q(h_t, a_t; \theta) = \sum_{i \in I} Q(h_t^i, a_t^i; \theta_i) \tag{9.44}
$$
<p>and</p>
$$
\max_{a \in A} Q(h_{t+1}, a; \bar{\theta}) = \sum_{i \in I} \max_{a_i \in A_i} Q(h_{t+1}^i, a_i; \bar{\theta}_i)
$$

            </div>
        </article></main>
</div>
<footer class="footer">
    <span class="footer_item"> </span>
    &nbsp;

    <div class="footer_social-icons">
<a href="https://github.com/AmineAndam04" target="_blank" rel="noopener noreferrer me"
    title="Github">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
</a>
<a href="https://twitter.com/AmineAndam" target="_blank" rel="noopener noreferrer me"
    title="Twitter">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z">
    </path>
</svg>
</a>
<a href="https://www.linkedin.com/in/amineandam/" target="_blank" rel="noopener noreferrer me"
    title="Linkedin">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
</a>
</div>
    <small class="footer_copyright">
        © 2024 Amine ANDAM.
        
    </small>
</footer><a href="#" title="" id="totop">
    <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" fill="currentColor" stroke="currentColor" viewBox="0 96 960 960">
    <path d="M283 704.739 234.261 656 480 410.261 725.739 656 677 704.739l-197-197-197 197Z"/>
</svg>

</a>


    






    
    <script src="https://amineandam04.github.io/js/main.min.35f435a5d8eac613c52daa28d8af544a4512337d3e95236e4a4978417b8dcb2f.js" integrity="sha256-NfQ1pdjqxhPFLaoo2K9USkUSM30&#43;lSNuSkl4QXuNyy8="></script>

    

</body>
</html>
